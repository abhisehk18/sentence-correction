{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_data_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2shcCHIYVHyi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87b45770-6120-46ee-8809-e1784e813c4c"
      },
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense,Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint \n",
        "import datetime\n",
        "import re\n",
        "import pickle\n",
        "#!pip install -U ipykernel\n",
        "from math import log\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "st.title(\" PREDICTING CORRECT FORM OF THE SENTENCE!\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "input_sentence=st.text_input('Enter The Corrupted Text')\n",
        "\n",
        "#st.title(input_sentence)\n",
        "\n",
        "submit=st.button(\"TRANSLATE!\")\n",
        "\n",
        "if submit:\n",
        "\n",
        "  with st.spinner(text=\"This may take a moment...\"):\n",
        "    class Encoder(tf.keras.Model):\n",
        "   \n",
        "      def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
        "        super().__init__()\n",
        "        #Initialize Embedding layer\n",
        "        #Intialize Encoder LSTM layer\n",
        "        self.embedding = Embedding(input_dim=inp_vocab_size, output_dim=embedding_size, input_length=input_length,\n",
        "                           mask_zero=True, name=\"embedding_layer_encoder\")\n",
        "        self.lstm = LSTM(lstm_size, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
        "\n",
        "      def call(self,input_sequence,initial_state):\n",
        "         \n",
        "        input_embedd                           = self.embedding(input_sequence)\n",
        "        self.lstm_output, self.state_h,self.state_c = self.lstm(input_embedd)\n",
        "        return self.lstm_output, self.state_h,self.state_c\n",
        "\n",
        "    \n",
        "      def initialize_states(self,batch_size):\n",
        "      \n",
        "        self.state_h=tf.zeros([batch_size])\n",
        "        self.state_c=tf.zeros([batch_size])\n",
        "        return self.state_h,self.state_c\n",
        "\n",
        "    \n",
        "    class Attention(tf.keras.layers.Layer):\n",
        "  \n",
        "      def __init__(self,scoring_function, att_units):\n",
        "\n",
        "        super().__init__()\n",
        "        self.att_units=att_units\n",
        "        self.scoring_function=scoring_function\n",
        "    # Please go through the reference notebook and research paper to complete the scoring functions\n",
        "\n",
        "        if self.scoring_function=='dot':\n",
        "      # Intialize variables needed for Dot score function here\n",
        "      \n",
        "          pass\n",
        "        if scoring_function == 'general':\n",
        "      # Intialize variables needed for General score function here\n",
        "          self.W=Dense(self.att_units)  \n",
        "      \n",
        "          pass\n",
        "        elif scoring_function == 'concat':\n",
        "      # Intialize variables needed for Concat score function here\n",
        "          self.W1=Dense(self.att_units,activation='tanh')\n",
        "          self.W2=Dense(self.att_units,activation='tanh')  \n",
        "    \n",
        "          self.V=Dense(1)\n",
        "          pass\n",
        "  \n",
        "  \n",
        "      def call(self,decoder_hidden_state,encoder_output):\n",
        "    \n",
        "    \n",
        "        if self.scoring_function == 'dot':\n",
        "        # Implement Dot score function here\n",
        "        #print(encoder_output.shape,decoder_hidden_state.shape)\n",
        "          decoder_hidden_state=tf.reshape(decoder_hidden_state,shape=(tf.shape(decoder_hidden_state)[0],1,tf.shape(decoder_hidden_state)[1]))\n",
        "        #print(decoder_hidden_state.shape[0])\n",
        "        #print(decoder_hidden_state.shape[1])\n",
        "        #print(decoder_hidden_state.shape)\n",
        "          score=tf.matmul(decoder_hidden_state,encoder_output,transpose_b=True)\n",
        "        \n",
        "        \n",
        "        elif self.scoring_function == 'general':\n",
        "        # Implement General score function here\n",
        "          decoder_hidden_state=tf.reshape(decoder_hidden_state,shape=(tf.shape(decoder_hidden_state)[0],1,tf.shape(decoder_hidden_state)[1]))\n",
        "          score=tf.matmul(decoder_hidden_state,self.W(encoder_output),transpose_b=True)\n",
        "       \n",
        "\n",
        "        elif self.scoring_function == 'concat':\n",
        "        # Implement General score function here\n",
        "          decoder=tf.expand_dims(decoder_hidden_state,1)\n",
        "          score=self.V(tf.nn.tanh(self.W1(decoder)+self.W2(encoder_output)))\n",
        "          attention_weights=tf.nn.softmax(score,axis=1)\n",
        "          context_vector=attention_weights*encoder_output\n",
        "          context_vector=tf.reduce_sum(context_vector,axis=1)\n",
        "\n",
        "          return context_vector, attention_weights\n",
        "\n",
        "        \n",
        "        attention_weights=tf.nn.softmax(score,axis=2)\n",
        "        context_vector=tf.matmul(attention_weights,encoder_output)\n",
        "        context_vector=tf.reshape(context_vector,shape=(tf.shape(context_vector)[0],tf.shape(context_vector)[2]))\n",
        "        attention_weights=tf.reshape(attention_weights,shape=(tf.shape(attention_weights)[0],tf.shape(attention_weights)[2],tf.shape(attention_weights)[1]))\n",
        "        return context_vector, attention_weights\n",
        "    \n",
        "    class One_Step_Decoder(tf.keras.Model):\n",
        "      def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
        "        super().__init__()\n",
        "      # Initialize decoder embedding layer, LSTM and any other objects needed\n",
        "        self.embedding = Embedding(input_dim=tar_vocab_size, output_dim=embedding_dim, input_length=input_length,\n",
        "                           mask_zero=True, name=\"embedding_layer\")\n",
        "        self.lstm=LSTM(dec_units,return_sequences=True,return_state=True)\n",
        "        self.attention=Attention(score_fun,att_units)\n",
        "        self.dense=tf.keras.layers.Dense(tar_vocab_size)\n",
        "\n",
        "\n",
        "      def call(self,input_to_decoder, encoder_output, state_h,state_c):\n",
        "   \n",
        "        x=self.embedding(input_to_decoder)\n",
        "        context_vector,attention_weights=self.attention(state_h,encoder_output)\n",
        "        concat=tf.concat([x,tf.expand_dims(context_vector,1)],axis=-1)\n",
        "        output,decoder_state_h,decoder_state_c=self.lstm(x,initial_state=[state_h,state_c])\n",
        "        output=tf.reshape(output,(-1,output.shape[2]))\n",
        "        final_output=self.dense(output)\n",
        "        return final_output,decoder_state_h,decoder_state_c,attention_weights,context_vector\n",
        "\n",
        "    class Decoder(tf.keras.Model):\n",
        "      def __init__(self,out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
        "      #Intialize necessary variables and create an object from the class onestepdecoder\n",
        "        super(Decoder,self).__init__()\n",
        "        self.out_vocab_size=out_vocab_size\n",
        "        self.embedding_dim=embedding_dim\n",
        "        self.input_length=input_length\n",
        "        self.dec_units=dec_units\n",
        "        self.score_fun=score_fun\n",
        "        self.att_units=att_units\n",
        "        self.One_Step_Decoder=One_Step_Decoder(self.out_vocab_size, self.embedding_dim, self.input_length, self.dec_units ,self.score_fun ,self.att_units)\n",
        "        \n",
        "      def call(self, input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state ):\n",
        "\n",
        "        #Initialize an empty Tensor array, that will store the outputs at each and every time step\n",
        "        #Create a tensor array as shown in the reference notebook\n",
        "        \n",
        "        #Iterate till the length of the decoder input\n",
        "            # Call onestepdecoder for each token in decoder_input\n",
        "            # Store the output in tensorarray\n",
        "        # Return the tensor array\n",
        "          all_outputs=tf.TensorArray(tf.float32,size=tf.shape(input_to_decoder)[1])\n",
        "          for i in range(tf.shape(input_to_decoder)[1]):\n",
        "            output,decoder_hidden_state,decoder_cell_state,attention_weights,context_vector=self.One_Step_Decoder(input_to_decoder[:,i:i+1],encoder_output,decoder_hidden_state,decoder_cell_state)\n",
        "          \n",
        "            all_outputs=all_outputs.write(i,output)\n",
        "          all_outputs=tf.transpose(all_outputs.stack(),[1,0,2])\n",
        "          return all_outputs\n",
        "\n",
        "    class encoder_decoder(tf.keras.Model):\n",
        "      def __init__(self,inp_vocab_size, embedding_dim,enc_units,input_length,out_vocab_size,dec_units ,score_fun ,att_units,batch_size):\n",
        "    #Intialize objects from encoder decoder\n",
        "        super().__init__()\n",
        "        self.batch_size=batch_size\n",
        "        self.encoder=Encoder(inp_vocab_size,embedding_dim,enc_units,input_length)\n",
        "        self.decoder=Decoder(out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units)\n",
        "\n",
        "  \n",
        "      def call(self,data):\n",
        "    #Intialize encoder states, Pass the encoder_sequence to the embedding layer\n",
        "    # Decoder initial states are encoder final states, Initialize it accordingly\n",
        "    # Pass the decoder sequence,encoder_output,decoder states to Decoder\n",
        "    # return the decoder output\n",
        "    \n",
        "        input,output=data[0],data[1]\n",
        "        initial_state=self.encoder.initialize_states(self.batch_size)\n",
        "        encoder_output,encoder_final_state_h,encoder_final_state_c=self.encoder(input,initial_state)\n",
        "        decoder_outputs=self.decoder(output,encoder_output,encoder_final_state_h,encoder_final_state_c)\n",
        "\n",
        "        return decoder_outputs\n",
        "\n",
        "    def load_models():\n",
        "  \n",
        "      train=pd.read_pickle('train.pkl')\n",
        "      validation=pd.read_pickle('validation.pkl')\n",
        "\n",
        "      with open('tok_corrupted.pickle', 'rb') as handle:\n",
        "        tok_corrupted = pickle.load(handle)\n",
        "\n",
        "\n",
        "      with open('tok_uncorrupted.pickle', 'rb') as handle1:\n",
        "        tok_uncorrupted = pickle.load(handle1)\n",
        "\n",
        "\n",
        "      vocab_corrupt=len(tok_corrupted.word_index.keys())\n",
        "      vocab_uncorrupt=len(tok_uncorrupted.word_index.keys())\n",
        "\n",
        "      train_dataset = Dataset(train, tok_corrupted, tok_uncorrupted,25 )\n",
        "      test_dataset  = Dataset(validation,tok_corrupted, tok_uncorrupted,25)\n",
        "\n",
        "\n",
        "      train_dataloader = Dataloder(train_dataset, batch_size=64)\n",
        "      test_dataloader = Dataloder(test_dataset, batch_size=64)\n",
        "\n",
        "\n",
        "      model  = encoder_decoder(vocab_corrupt,100,300,25,vocab_uncorrupt,300,'concat',50,64)\n",
        "      optimizer = tf.keras.optimizers.Adam()\n",
        "      model.compile(optimizer=optimizer,loss=loss_function,metrics=[accuracy])\n",
        "      batch_size=64\n",
        "  \n",
        "      model.fit_generator(train_dataloader, steps_per_epoch=1, epochs=1, validation_data=test_dataloader, validation_steps=1)\n",
        "      model.load_weights('model.h5')\n",
        "\n",
        "      return model\n",
        "\n",
        "    def loss_function(real, pred):\n",
        "    \n",
        "    \n",
        "    \n",
        "      mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "      loss_ = loss_object(real, pred)\n",
        "\n",
        "      mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "      loss_ *= mask\n",
        "\n",
        "      return tf.reduce_mean(loss_)\n",
        "    #@st.cache\n",
        "    class Dataset:\n",
        "      def __init__(self, data, tok_corrupt, tok_uncorrupt, max_len):\n",
        "        self.encoder_inps = data['corrupted'].values\n",
        "        self.decoder_inps = data['uncorrupted_inp'].values\n",
        "        self.decoder_outs = data['uncorrupted_op'].values\n",
        "        self.tok_corrupt = tok_corrupt\n",
        "        self.tok_uncorrupt = tok_uncorrupt\n",
        "        self.max_len = max_len\n",
        "\n",
        "      def __getitem__(self, i):\n",
        "        self.encoder_seq = self.tok_corrupt.texts_to_sequences([self.encoder_inps[i]]) # need to pass list of values\n",
        "        self.decoder_inp_seq = self.tok_uncorrupt.texts_to_sequences([self.decoder_inps[i]])\n",
        "        self.decoder_out_seq = self.tok_uncorrupt.texts_to_sequences([self.decoder_outs[i]])\n",
        "\n",
        "        self.encoder_seq = pad_sequences(self.encoder_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
        "        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
        "        self.decoder_out_seq = pad_sequences(self.decoder_out_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
        "        return self.encoder_seq, self.decoder_inp_seq, self.decoder_out_seq\n",
        "\n",
        "      def __len__(self): # your model.fit_gen requires this function\n",
        "        return len(self.encoder_inps)\n",
        "\n",
        "    \n",
        "    class Dataloder(tf.keras.utils.Sequence):    \n",
        "      def __init__(self, dataset, batch_size=1):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.indexes = np.arange(len(self.dataset.encoder_inps))\n",
        "\n",
        "      def __getitem__(self, i):\n",
        "        start = i * self.batch_size\n",
        "        stop = (i + 1) * self.batch_size\n",
        "        data = []\n",
        "        for j in range(start, stop):\n",
        "            data.append(self.dataset[j])\n",
        "\n",
        "        batch = [np.squeeze(np.stack(samples, axis=1), axis=0) for samples in zip(*data)]\n",
        "        return tuple([[batch[0],batch[1]],batch[2]])\n",
        "\n",
        "      def __len__(self):  # your model.fit_gen requires this function\n",
        "        return len(self.indexes) // self.batch_size\n",
        "\n",
        "      def on_epoch_end(self):\n",
        "        self.indexes = np.random.permutation(self.indexes)\n",
        "\n",
        "\n",
        "    def beam_search_decoder(data, k):\n",
        "      sequences = [[list(), 0.0]]\n",
        "      for row in data:\n",
        "        all_candidates = list()\n",
        "        for i in range(len(sequences)):\n",
        "          seq, score = sequences[i]\n",
        "          for j in range(len(row)):\n",
        "            try:\n",
        "              candidate = [seq + [j], score - log(row[j])]\n",
        "              all_candidates.append(candidate)\n",
        "            except ValueError as e:\n",
        "              candidate = [seq + [j], 0]\n",
        "              all_candidates.append(candidate)\n",
        "      # order all candidates by score\n",
        "        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "    # select k best\n",
        "        sequences = ordered[:k]\n",
        "      return sequences\n",
        "\n",
        "    from tensorflow.keras import backend as K\n",
        "    def accuracy(y_true, y_pred):\n",
        "\n",
        "      pred_value= K.cast(K.argmax(y_pred, axis=-1), dtype='float32')\n",
        "      true_value = K.cast(K.equal(y_true, pred_value), dtype='float32')\n",
        "\n",
        "      mask = K.cast(K.greater(y_true, 0), dtype='float32')\n",
        "      n_correct = K.sum(mask * true_value)\n",
        "      n_total = K.sum(mask)\n",
        "  \n",
        "      return n_correct / n_total\n",
        "\n",
        "    def prediction(input_sentence):\n",
        "\n",
        "\n",
        "      with open('tok_uncorrupted.pickle', 'rb') as handle1:\n",
        "        tok_uncorrupted = pickle.load(handle1)\n",
        "\n",
        "      with open('tok_corrupted.pickle', 'rb') as handle:\n",
        "        tok_corrupted = pickle.load(handle)\n",
        "  \n",
        "      model=load_models()\n",
        "\n",
        "  \n",
        " \n",
        "      input_vec = tok_corrupted.texts_to_sequences([input_sentence])\n",
        "      input_vec =  tf.keras.preprocessing.sequence.pad_sequences(input_vec,maxlen=25,padding='post')\n",
        "      input_vec = tf.convert_to_tensor(input_vec)\n",
        "      enc_initial_states = None\n",
        "      enc_out, h_state, c_state = model.layers[0](input_vec,enc_initial_states)\n",
        "      curr_vec = np.array(tok_uncorrupted.word_index[\"<start>\"])\n",
        "      curr_vec = curr_vec.reshape(1,1)\n",
        "      end_vec = np.array(tok_uncorrupted.word_index[\"<end>\"]).reshape(1,1)\n",
        "      result=''\n",
        "      atten_weights= np.zeros((25,25)) #(max_target_length, max_source_length)\n",
        "      for i in range(25):\n",
        "        decoder_output, lstm_state_h, lstm_state_c, atten_w, context_vectors = model.layers[1].One_Step_Decoder(curr_vec, enc_out, h_state, c_state)\n",
        "    \n",
        "        Result_beam_list=beam_search_decoder(decoder_output,k=1)\n",
        "        Result_beam=Result_beam_list[0][0]\n",
        "   \n",
        "        predicted_id = tf.argmax(decoder_output[0]).numpy()\n",
        "      \n",
        "    \n",
        "        if tok_uncorrupted.index_word[predicted_id] != '<end>':\n",
        "          result += tok_uncorrupted.index_word[Result_beam[0]] + ' '\n",
        "      # the predicted ID is fed back into the model\n",
        "      \n",
        "        h_state, c_state = lstm_state_h, lstm_state_c\n",
        "        curr_vec = tf.expand_dims([predicted_id], 0)\n",
        "  \n",
        "\n",
        "      return result\n",
        "    #result, input_sentence, attention_plot = predict(inp_sentence)\n",
        "    #print('Input: %s' % (input_sentence))\n",
        "    #print(\"-\"*50)\n",
        "    ##print('Predicted translation: {}'.format(result))\n",
        "    #print(\"-\"*50)\n",
        "    \n",
        "    #return result\n",
        "\n",
        "  st.text_area('Translated Sentence:',prediction(input_sentence))\n",
        "  \n",
        "\n",
        "st.markdown('''\n",
        "    <a href=\"https://www.linkedin.com/in/abhishek-rai-8b3634169/\">\n",
        "        <img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGQAAABkCAMAAABHPGVmAAAAaVBMVEUAd7X///8AcLEEebb7/f4Debb9/v79/v8DeLY8l8Y7lsYAbbCcyuKlz+WQxN+Lwd0KfbiDvdvl8fcAc7O+3OwgiL6x1ehFnMlytNZqsNQZhLzF4O7w9/tSo81gqtGp0ebc7PUsjsJ8udngaOvIAAADg0lEQVRoge2abZeqIBCAGV7EjXItKjc1y/7/j7xD7m5AWHdD+uScs3s8kDzOIMwMDoGrSJ4lEC6H0ckVkUEiyeQPBC+ar+pjcqm+GsQMEAntUSuaQJQ+toZC8F+7U4KJBMKE6ltUwkAqJUgiEapCAJGwpSwVgxBGtyAJh4NKCVEH4AStRZNZC+1F0V5orgXN00FyukBzZekh2XsgbzFXCMIGSQlhTCsjegrMCEQQVXfF8rOrFYm3YxjCdFnw6zbdFkJHr6AgROjdGYAbATjXOlaXEISRcgN88GEZh3NJIuclBBF0BfzXr3EoYve1ACTXPViuOAO+izRYAMLUxVLEqBK7RQcggp48yHJ6SG5cjAWRsI/0A0FN9g6Ew2p6TZhae+a6pDDXwn67UKt++rcL3+Gts072sW4gDOn5L4VjRBa7rwT3LkarDMNvmeEf8EV0tDSyC9O++dZku4uPyEb8CdO6KrbNtvjQOt5rjXnGnGGkrKmibALfP+7jBQ6fs0lCvnHIn4Tlj2KBsTkJX4cGYgLH1yiD8n+A/LcgACevrHe7ujRXQYXCEFHaIsLNZjgMnMrjstm0nLebZnXodSi6Cbvfz3Zzk3Y97I/ozOxmE2Dkql5vnCS0OQbe+WAgoZbOjcUPpLBbeU0I7dprZi4xlZYyM8ENNP3d6h2DcPkj/Ab5vDVjkllrgg/DpfNA+HN59D3DKMTahW3ILbxoa3Ru/D75x59Uni6vQ3hZOL7N8j9+ePMqRMLmACNnGHce6FUItrcgA4ThCdwU9HUIjDLuVImAjB/4+EFnjCZXba6xv/SU8oLOOEhmvQgu5GRPShQER25Pl667nFrXeBmc7ZUSA8FxLyU1mSWtTw7Fm5S4ia/Ud5KsnVtM34dlrwiIxNkV31ZhhG1sXTh01sy/DkG7W55SqC9npcLXJBBvHEzPLFU4rCeBgBeHs7MDKaaA4A5ZWggvdXKzs5chGTSOn2XKTZlX02jiJnnGOU8O8ZO8u146CWTpQdYzZIbMkBkyQ2bIDJkhA+QmDsRqDkCs3tXTqH4FtiyDxx6BkMiW/ZO4K6eH/eom+27oy2nnNB+8DPfo9j5JgswBk/PVnj1sfnzTKMScPVpf7fMnzU97JzqDfCxvhMxfsf8Ake8pX3hLIUbqkhJtSkpSF8ccTXGMKSXqk5f5vKVg6Xpkkbz06j1FZJC6HO4fbRVnSvd35FUAAAAASUVORK5CYII=\" />\n",
        "    </a>''',\n",
        "    unsafe_allow_html=True\n",
        ")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-hJU7KEBC1G",
        "outputId": "21aecbec-31b7-4810-cc64-9a61f17ce286"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |▉                               | 20 kB 34.0 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███                             | 71 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 81 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 92 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 102 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 112 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 122 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 133 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 143 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 153 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████                         | 163 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 174 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 184 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 194 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 204 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 215 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 225 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 235 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 245 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 256 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 266 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 276 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 286 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 296 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 307 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 317 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 327 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 337 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 348 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 358 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 368 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 378 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 389 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 399 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 409 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 419 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 430 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 440 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 450 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 460 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 471 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 481 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 491 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 501 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 512 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 522 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 532 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 542 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 552 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 563 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 573 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 583 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 593 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 604 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 614 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 624 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 634 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 645 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 655 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 665 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 675 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 686 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 696 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 706 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 716 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 727 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 737 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 745 kB 7.4 MB/s \n",
            "\u001b[?25h  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 23Uk3EbRKXFi66Ci23h6OjE1Uck_eikTGoay8PBxbVo6JBX7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_118AzzpAs3L",
        "outputId": "7b11d5ed-5f82-40fc-95b3-d3628212a12a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(port='80')\n",
        "print (public_url)\n",
        "!streamlit run --server.port 80 app.py >/dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-r-n38OAvfj",
        "outputId": "3b59f284-37d4-4168-b3e8-115a4deaab32"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-01-13 08:23:41.920 INFO    pyngrok.ngrok: Opening tunnel named: http-80-be8e3363-3453-44f3-bf35-661af3541e96\n",
            "2022-01-13 08:23:41.959 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:41+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "2022-01-13 08:23:41.961 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:41+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.ngrok2/ngrok.yml\n",
            "2022-01-13 08:23:41.971 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:41+0000 lvl=info msg=\"open config file\" path=/root/.ngrok2/ngrok.yml err=nil\n",
            "2022-01-13 08:23:41.972 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:41+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040\n",
            "2022-01-13 08:23:42.190 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:42+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "2022-01-13 08:23:42.192 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:42+0000 lvl=info msg=\"client session established\" obj=csess id=07b6aa9f503d\n",
            "2022-01-13 08:23:42.199 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:42+0000 lvl=info msg=start pg=/api/tunnels id=18061a8697a83b10\n",
            "2022-01-13 08:23:42.200 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:42+0000 lvl=info msg=end pg=/api/tunnels id=18061a8697a83b10 status=200 dur=487.6µs\n",
            "2022-01-13 08:23:42.204 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:42+0000 lvl=info msg=start pg=/api/tunnels id=9b3b85c4b9f19d44\n",
            "2022-01-13 08:23:42.206 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:42+0000 lvl=info msg=end pg=/api/tunnels id=9b3b85c4b9f19d44 status=200 dur=283.057µs\n",
            "2022-01-13 08:23:42.209 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:42+0000 lvl=info msg=start pg=/api/tunnels id=e39a7e2944f6d615\n",
            "2022-01-13 08:23:42.379 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:42+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=\"http-80-be8e3363-3453-44f3-bf35-661af3541e96 (http)\" addr=http://localhost:80 url=http://41a4-34-125-196-62.ngrok.io\n",
            "2022-01-13 08:23:42.385 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:42+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-80-be8e3363-3453-44f3-bf35-661af3541e96 addr=http://localhost:80 url=https://41a4-34-125-196-62.ngrok.io\n",
            "2022-01-13 08:23:42.399 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:42+0000 lvl=info msg=end pg=/api/tunnels id=e39a7e2944f6d615 status=201 dur=174.700105ms\n",
            "2022-01-13 08:23:42.401 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:42+0000 lvl=info msg=start pg=\"/api/tunnels/http-80-be8e3363-3453-44f3-bf35-661af3541e96 (http)\" id=2bc6eae4c4c8bb57\n",
            "2022-01-13 08:23:42.403 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:42+0000 lvl=info msg=end pg=\"/api/tunnels/http-80-be8e3363-3453-44f3-bf35-661af3541e96 (http)\" id=2bc6eae4c4c8bb57 status=200 dur=228.76µs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NgrokTunnel: \"http://41a4-34-125-196-62.ngrok.io\" -> \"http://localhost:80\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-01-13 08:23:47.417 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:47+0000 lvl=info msg=\"join connections\" obj=join id=01f08208b49d l=127.0.0.1:80 r=124.108.16.134:51066\n",
            "2022-01-13 08:23:48.832 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:48+0000 lvl=info msg=\"join connections\" obj=join id=b762dce3f8e9 l=127.0.0.1:80 r=124.108.16.134:50786\n",
            "2022-01-13 08:23:48.837 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:48+0000 lvl=info msg=\"join connections\" obj=join id=8110813c16cf l=127.0.0.1:80 r=124.108.16.134:57020\n",
            "2022-01-13 08:23:48.858 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:48+0000 lvl=info msg=\"join connections\" obj=join id=39254534b280 l=127.0.0.1:80 r=124.108.16.134:57022\n",
            "2022-01-13 08:23:48.897 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:48+0000 lvl=info msg=\"join connections\" obj=join id=04946d74fd75 l=127.0.0.1:80 r=124.108.16.134:51070\n",
            "2022-01-13 08:23:48.923 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:48+0000 lvl=info msg=\"join connections\" obj=join id=82d407a20d40 l=127.0.0.1:80 r=124.108.16.134:53214\n",
            "2022-01-13 08:23:50.803 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:50+0000 lvl=info msg=\"join connections\" obj=join id=7dc5ca1eddd1 l=127.0.0.1:80 r=124.108.16.134:45302\n",
            "2022-01-13 08:23:53.155 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:53+0000 lvl=info msg=\"join connections\" obj=join id=bba4b8e705b1 l=127.0.0.1:80 r=223.236.226.26:60726\n",
            "2022-01-13 08:23:53.482 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:53+0000 lvl=info msg=\"join connections\" obj=join id=7bccf6cf3a19 l=127.0.0.1:80 r=223.236.226.26:60729\n",
            "2022-01-13 08:23:53.503 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:53+0000 lvl=info msg=\"join connections\" obj=join id=8086f38c7266 l=127.0.0.1:80 r=223.236.226.26:60728\n",
            "2022-01-13 08:23:53.526 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:53+0000 lvl=info msg=\"join connections\" obj=join id=a2dccc437209 l=127.0.0.1:80 r=223.236.226.26:60727\n",
            "2022-01-13 08:23:57.651 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:57+0000 lvl=info msg=\"join connections\" obj=join id=103dc9dd650a l=127.0.0.1:80 r=223.236.226.26:60732\n",
            "2022-01-13 08:23:58.110 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:58+0000 lvl=info msg=\"join connections\" obj=join id=935c8cd71a54 l=127.0.0.1:80 r=223.236.226.26:60724\n",
            "2022-01-13 08:23:58.146 INFO    pyngrok.process.ngrok: t=2022-01-13T08:23:58+0000 lvl=info msg=\"join connections\" obj=join id=e19cd8748c4d l=127.0.0.1:80 r=223.236.226.26:60730\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-13 08:24:10.772167: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "/content/app.py:224: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(train_dataloader, steps_per_epoch=1, epochs=1, validation_data=test_dataloader, validation_steps=1)\n",
            "/content/app.py:224: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(train_dataloader, steps_per_epoch=1, epochs=1, validation_data=test_dataloader, validation_steps=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-01-13 08:26:36.547 INFO    pyngrok.process.ngrok: t=2022-01-13T08:26:36+0000 lvl=info msg=\"received stop request\" obj=app stopReq=\"{err:<nil> restart:false}\"\n",
            "2022-01-13 08:26:36.551 INFO    pyngrok.process.ngrok: t=2022-01-13T08:26:36+0000 lvl=info msg=\"session closing\" obj=tunnels.session err=nil\n"
          ]
        }
      ]
    }
  ]
}