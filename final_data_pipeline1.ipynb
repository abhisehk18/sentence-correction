{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_data_pipeline.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym-oho4qmXSU"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#!pip3 install nlpaug\n",
        "import tarfile\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# import seaborn as sns\n",
        "import pandas as pd\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense,Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint \n",
        "import numpy as np\n",
        "import datetime\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii3LueNYRk_E"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#!pip3 install nlpaug\n",
        "import tarfile"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmkgWe0uvbat",
        "outputId": "11e213bb-9091-40b3-bcfc-c7fefe7fd9dc"
      },
      "source": [
        "!wget https://www.dropbox.com/s/ddkmtqz01jc024u/glove.6B.100d.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-08 09:10:13--  https://www.dropbox.com/s/ddkmtqz01jc024u/glove.6B.100d.txt\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/ddkmtqz01jc024u/glove.6B.100d.txt [following]\n",
            "--2021-11-08 09:10:13--  https://www.dropbox.com/s/raw/ddkmtqz01jc024u/glove.6B.100d.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc3725d4c711009ad5a98b27fff3.dl.dropboxusercontent.com/cd/0/inline/BZmQqumjcyGaMRWuwvKIQnzVK2hS1NwPc-7T-ZFq4hUvz0myA0hn3huQVEmCTuMhmVMydGIY9_ONqQrgiL87RRg2DS8lDS9d0kaxPAPbaKfaQyxZUvLEY2CamO7fBoX1jE7DaHqoY5keMebYXjfw9IHb/file# [following]\n",
            "--2021-11-08 09:10:13--  https://uc3725d4c711009ad5a98b27fff3.dl.dropboxusercontent.com/cd/0/inline/BZmQqumjcyGaMRWuwvKIQnzVK2hS1NwPc-7T-ZFq4hUvz0myA0hn3huQVEmCTuMhmVMydGIY9_ONqQrgiL87RRg2DS8lDS9d0kaxPAPbaKfaQyxZUvLEY2CamO7fBoX1jE7DaHqoY5keMebYXjfw9IHb/file\n",
            "Resolving uc3725d4c711009ad5a98b27fff3.dl.dropboxusercontent.com (uc3725d4c711009ad5a98b27fff3.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:601b:15::a27d:80f\n",
            "Connecting to uc3725d4c711009ad5a98b27fff3.dl.dropboxusercontent.com (uc3725d4c711009ad5a98b27fff3.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 347116733 (331M) [text/plain]\n",
            "Saving to: ‘glove.6B.100d.txt’\n",
            "\n",
            "glove.6B.100d.txt   100%[===================>] 331.04M  60.3MB/s    in 5.5s    \n",
            "\n",
            "2021-11-08 09:10:19 (60.2 MB/s) - ‘glove.6B.100d.txt’ saved [347116733/347116733]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNtJfm-31UFS",
        "outputId": "281aeb65-ce4f-4f9e-b7f2-0568465a548a"
      },
      "source": [
        "!curl --header \"Host: www.cl.cam.ac.uk\" --header \"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36\" --header \"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header \"Accept-Language: en-GB,en;q=0.9\" --header \"Referer: https://www.cl.cam.ac.uk/research/nl/bea2019st/\" \"https://www.cl.cam.ac.uk/research/nl/bea2019st/data/fce_v2.1.bea19.tar.gz\" -L -o \"fce_v2.1.bea19.tar.gz\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2709k  100 2709k    0     0  2347k      0  0:00:01  0:00:01 --:--:-- 2347k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Zcb_CFF2Z93"
      },
      "source": [
        "import tarfile\n",
        "fname='/content/fce_v2.1.bea19.tar.gz'\n",
        "fname.endswith(\"tar.gz\")\n",
        "tar = tarfile.open(fname, \"r:gz\")\n",
        "tar.extractall()\n",
        "tar.close()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chuB4Ldt1VJj",
        "outputId": "ca89bdbd-dda3-4282-a231-dc5045e3db1e"
      },
      "source": [
        "!curl --header \"Host: www.cl.cam.ac.uk\" --header \"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36\" --header \"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header \"Accept-Language: en-GB,en;q=0.9\" --header \"Referer: https://www.cl.cam.ac.uk/research/nl/bea2019st/\" \"https://www.cl.cam.ac.uk/research/nl/bea2019st/data/wi+locness_v2.1.bea19.tar.gz\" -L -o \"wi+locness_v2.1.bea19.tar.gz\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 5977k  100 5977k    0     0  4541k      0  0:00:01  0:00:01 --:--:-- 4541k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj53m8oo2e2t"
      },
      "source": [
        "import tarfile\n",
        "fname='/content/wi+locness_v2.1.bea19.tar.gz'\n",
        "fname.endswith(\"tar.gz\")\n",
        "tar = tarfile.open(fname, \"r:gz\")\n",
        "tar.extractall()\n",
        "tar.close()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u03CLv1fTF5v"
      },
      "source": [
        "def m2_to_txt(path):\n",
        "  '''\n",
        "  this function processes the .m2 format file \n",
        "  and converts the required sentence into txt format'''\n",
        "  m2 = open(path).read().strip().split(\"\\n\\n\")\n",
        "  corrupted = open('corrupted.txt', \"w\")\n",
        "  uncorrupted=open('uncorrupted.txt','w')\n",
        "  skip = {\"noop\", \"UNK\", \"Um\"}\n",
        "  for sent in m2:\n",
        "    sent=sent.split('\\n')\n",
        "  \n",
        "    incorr_sent=sent[0].split()[1:]\n",
        "  \n",
        "    corrupted.write(\" \".join(incorr_sent)+\"\\n\")\n",
        "    edits = sent[1:]\n",
        "  \n",
        "    offset=0\n",
        "  \n",
        "    for edit in edits:\n",
        "\t\t    edit = edit.split(\"|||\")\n",
        "\t\t    if edit[1] in skip: continue # Ignore certain edits\n",
        "\t\t\t\n",
        "\t\t\t\n",
        "\t\t    span = edit[0].split()[1:] # Ignore \"A \"\n",
        "\t\t    start = int(span[0])\n",
        "\t\t    end = int(span[1])\n",
        "\t\t    cor = edit[2].split()\n",
        "\t\t    incorr_sent[start+offset:end+offset] = cor\n",
        "      \n",
        "\t\t    offset = offset-(end-start)+len(cor)\n",
        "      \n",
        "    uncorrupted.write(\" \".join(incorr_sent)+\"\\n\")\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jntgzbwokjG"
      },
      "source": [
        "#extracting data from m2 files\n",
        "m2_to_txt('/content/fce/m2/fce.dev.gold.bea19.m2')\n",
        "with open('uncorrupted.txt','r') as file:\n",
        "  uncorrupted=[]\n",
        "  line=file.readlines()\n",
        "  for lines in line:\n",
        "    uncorrupted.append(\" \".join(lines.split()[:len(lines)]))\n",
        "with open('corrupted.txt','r') as file:\n",
        "  corrupted=[]\n",
        "  line=file.readlines()\n",
        "  for lines in line:\n",
        "    corrupted.append(\" \".join(lines.split()[:len(lines)]))\n",
        "\n",
        "data1=pd.DataFrame(zip(corrupted,uncorrupted),columns=['corrupted','uncorrupted'])\n",
        "\n",
        "m2_to_txt('/content/fce/m2/fce.train.gold.bea19.m2')\n",
        "with open('uncorrupted.txt','r') as file:\n",
        "  uncorrupted=[]\n",
        "  line=file.readlines()\n",
        "  for lines in line:\n",
        "    uncorrupted.append(\" \".join(lines.split()[:len(lines)]))\n",
        "with open('corrupted.txt','r') as file:\n",
        "  corrupted=[]\n",
        "  line=file.readlines()\n",
        "  for lines in line:\n",
        "    corrupted.append(\" \".join(lines.split()[:len(lines)]))\n",
        "\n",
        "data2=pd.DataFrame(zip(corrupted,uncorrupted),columns=['corrupted','uncorrupted'])\n",
        "\n",
        "m2_to_txt('/content/wi+locness/m2/N.dev.gold.bea19.m2')\n",
        "with open('uncorrupted.txt','r') as file:\n",
        "  uncorrupted=[]\n",
        "  line=file.readlines()\n",
        "  for lines in line:\n",
        "    uncorrupted.append(\" \".join(lines.split()[:len(lines)]))\n",
        "with open('corrupted.txt','r') as file:\n",
        "  corrupted=[]\n",
        "  line=file.readlines()\n",
        "  for lines in line:\n",
        "    corrupted.append(\" \".join(lines.split()[:len(lines)]))\n",
        "\n",
        "data3=pd.DataFrame(zip(corrupted,uncorrupted),columns=['corrupted','uncorrupted'])\n",
        "\n",
        "m2_to_txt('/content/wi+locness/m2/C.dev.gold.bea19.m2')\n",
        "with open('uncorrupted.txt','r') as file:\n",
        "  uncorrupted=[]\n",
        "  line=file.readlines()\n",
        "  for lines in line:\n",
        "    uncorrupted.append(\" \".join(lines.split()[:len(lines)]))\n",
        "with open('corrupted.txt','r') as file:\n",
        "  corrupted=[]\n",
        "  line=file.readlines()\n",
        "  for lines in line:\n",
        "    corrupted.append(\" \".join(lines.split()[:len(lines)]))\n",
        "\n",
        "data4=pd.DataFrame(zip(corrupted,uncorrupted),columns=['corrupted','uncorrupted'])\n",
        "\n",
        "m2_to_txt('/content/wi+locness/m2/B.dev.gold.bea19.m2')\n",
        "with open('uncorrupted.txt','r') as file:\n",
        "  uncorrupted=[]\n",
        "  line=file.readlines()\n",
        "  for lines in line:\n",
        "    uncorrupted.append(\" \".join(lines.split()[:len(lines)]))\n",
        "with open('corrupted.txt','r') as file:\n",
        "  corrupted=[]\n",
        "  line=file.readlines()\n",
        "  for lines in line:\n",
        "    corrupted.append(\" \".join(lines.split()[:len(lines)]))\n",
        "\n",
        "data5=pd.DataFrame(zip(corrupted,uncorrupted),columns=['corrupted','uncorrupted'])\n",
        "\n",
        "m2_to_txt('/content/wi+locness/m2/ABCN.dev.gold.bea19.m2')\n",
        "with open('uncorrupted.txt','r') as file:\n",
        "  uncorrupted=[]\n",
        "  line=file.readlines()\n",
        "  for lines in line:\n",
        "    uncorrupted.append(\" \".join(lines.split()[:len(lines)]))\n",
        "with open('corrupted.txt','r') as file:\n",
        "  corrupted=[]\n",
        "  line=file.readlines()\n",
        "  for lines in line:\n",
        "    corrupted.append(\" \".join(lines.split()[:len(lines)]))\n",
        "\n",
        "data6=pd.DataFrame(zip(corrupted,uncorrupted),columns=['corrupted','uncorrupted'])\n",
        "\n",
        "m2_to_txt('/content/wi+locness/m2/A.dev.gold.bea19.m2')\n",
        "with open('uncorrupted.txt','r') as file:\n",
        "  uncorrupted=[]\n",
        "  line=file.readlines()\n",
        "  for lines in line:\n",
        "    uncorrupted.append(\" \".join(lines.split()[:len(lines)]))\n",
        "with open('corrupted.txt','r') as file:\n",
        "  corrupted=[]\n",
        "  line=file.readlines()\n",
        "  for lines in line:\n",
        "    corrupted.append(\" \".join(lines.split()[:len(lines)]))\n",
        "\n",
        "data7=pd.DataFrame(zip(corrupted,uncorrupted),columns=['corrupted','uncorrupted'])\n",
        "\n",
        "m2_to_txt('/content/wi+locness/m2/A.train.gold.bea19.m2')\n",
        "with open('uncorrupted.txt','r') as file:\n",
        "  uncorrupted=[]\n",
        "  line=file.readlines()\n",
        "  for lines in line:\n",
        "    uncorrupted.append(\" \".join(lines.split()[:len(lines)]))\n",
        "with open('corrupted.txt','r') as file:\n",
        "  corrupted=[]\n",
        "  line=file.readlines()\n",
        "  for lines in line:\n",
        "    corrupted.append(\" \".join(lines.split()[:len(lines)]))\n",
        "\n",
        "data8=pd.DataFrame(zip(corrupted,uncorrupted),columns=['corrupted','uncorrupted'])\n",
        "\n",
        "\n",
        "m2_to_txt('/content/wi+locness/m2/B.train.gold.bea19.m2')\n",
        "with open('uncorrupted.txt','r') as file:\n",
        "  uncorrupted=[]\n",
        "  line=file.readlines()\n",
        "  for lines in line:\n",
        "    uncorrupted.append(\" \".join(lines.split()[:len(lines)]))\n",
        "with open('corrupted.txt','r') as file:\n",
        "  corrupted=[]\n",
        "  line=file.readlines()\n",
        "  for lines in line:\n",
        "    corrupted.append(\" \".join(lines.split()[:len(lines)]))\n",
        "\n",
        "data9=pd.DataFrame(zip(corrupted,uncorrupted),columns=['corrupted','uncorrupted'])\n",
        "\n",
        "\n",
        "m2_to_txt('/content/wi+locness/m2/ABC.train.gold.bea19.m2')\n",
        "with open('uncorrupted.txt','r') as file:\n",
        "  uncorrupted=[]\n",
        "  line=file.readlines()\n",
        "  for lines in line:\n",
        "    uncorrupted.append(\" \".join(lines.split()[:len(lines)]))\n",
        "with open('corrupted.txt','r') as file:\n",
        "  corrupted=[]\n",
        "  line=file.readlines()\n",
        "  for lines in line:\n",
        "    corrupted.append(\" \".join(lines.split()[:len(lines)]))\n",
        "\n",
        "data10=pd.DataFrame(zip(corrupted,uncorrupted),columns=['corrupted','uncorrupted'])\n",
        "\n",
        "\n",
        "m2_to_txt('/content/wi+locness/m2/C.train.gold.bea19.m2')\n",
        "with open('uncorrupted.txt','r') as file:\n",
        "  uncorrupted=[]\n",
        "  line=file.readlines()\n",
        "  for lines in line:\n",
        "    uncorrupted.append(\" \".join(lines.split()[:len(lines)]))\n",
        "with open('corrupted.txt','r') as file:\n",
        "  corrupted=[]\n",
        "  line=file.readlines()\n",
        "  for lines in line:\n",
        "    corrupted.append(\" \".join(lines.split()[:len(lines)]))\n",
        "\n",
        "data11=pd.DataFrame(zip(corrupted,uncorrupted),columns=['corrupted','uncorrupted'])\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SuTgmY1sDDi"
      },
      "source": [
        "#compiling all the datasets into one\n",
        "data=pd.concat([data1, data2, data3, data4, data5, data6, data7, data8, data9, data10, data11],ignore_index=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLoSC1yyt6io"
      },
      "source": [
        "#removing rows having same data in both columns\n",
        "data=data[data['corrupted']!=data['uncorrupted']]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk0Ni1PMT23K"
      },
      "source": [
        "#https://stackoverflow.com/questions/30887979/i-want-to-create-a-script-for-unzip-tar-gz-file-via-python\n",
        "import tarfile\n",
        "fname='sm_norm_mt.tar.gz'\n",
        "fname.endswith(\"tar.gz\")\n",
        "tar = tarfile.open(fname, \"r:gz\")\n",
        "tar.extractall()\n",
        "tar.close()\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ABO_KI8W851"
      },
      "source": [
        "#https://stackoverflow.com/questions/53987653/read-a-text-file-contents-from-second-line-in-python\n",
        "with open('/content/release/en2cn-2k.en2nen2cn','r') as file:\n",
        "  line=file.readlines()\n",
        "corrupted=[]\n",
        "uncorrupted=[]\n",
        "for i,lines in enumerate(line):\n",
        "  if i%3==0:\n",
        "    corrupted.append(lines.strip())\n",
        "  elif (i%3==1):\n",
        "    uncorrupted.append(lines.strip())\n",
        "  else:\n",
        "    pass\n",
        "  \n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l53RqCKSfA5A"
      },
      "source": [
        "data2=pd.DataFrame(zip(corrupted,uncorrupted),columns=['corrupted','uncorrupted'])\n",
        "#data=pd.concat([data,data12],ignore_index=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFuK2DJD5CT_"
      },
      "source": [
        "data1= data.sample(frac=1).reset_index(drop=True)\n",
        "data1=data1.drop_duplicates()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPAmvfDpti8R"
      },
      "source": [
        "data1.to_pickle('data1.pkl')\n",
        "data2.to_pickle('data2.pkl')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzXLb004fMrP"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    '''\n",
        "    Encoder model -- That takes a input sequence and returns output sequence\n",
        "    '''\n",
        "\n",
        "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
        "        super().__init__()\n",
        "        #Initialize Embedding layer\n",
        "        #Intialize Encoder LSTM layer\n",
        "        self.embedding = Embedding(input_dim=inp_vocab_size, output_dim=embedding_size, input_length=input_length,\n",
        "                           mask_zero=True, name=\"embedding_layer_encoder\")\n",
        "        self.lstm = LSTM(lstm_size, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
        "\n",
        "    def call(self,input_sequence,initial_state):\n",
        "          '''\n",
        "          This function takes a sequence input and the initial states of the encoder.\n",
        "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n",
        "          returns -- All encoder_outputs, last time steps hidden and cell state\n",
        "\n",
        "          '''\n",
        "          input_embedd                           = self.embedding(input_sequence)\n",
        "          self.lstm_output, self.state_h,self.state_c = self.lstm(input_embedd)\n",
        "          return self.lstm_output, self.state_h,self.state_c\n",
        "\n",
        "    \n",
        "    def initialize_states(self,batch_size):\n",
        "      '''\n",
        "      Given a batch size it will return intial hidden state and intial cell state.\n",
        "      If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n",
        "      '''\n",
        "      self.state_h=tf.zeros([batch_size])\n",
        "      self.state_c=tf.zeros([batch_size])\n",
        "      return self.state_h,self.state_c\n",
        "      "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3GFJhtlZfw5"
      },
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "  '''\n",
        "    Class the calculates score based on the scoring_function using Bahdanu attention mechanism.\n",
        "  '''\n",
        "  def __init__(self,scoring_function, att_units):\n",
        "\n",
        "    super().__init__()\n",
        "    self.att_units=att_units\n",
        "    self.scoring_function=scoring_function\n",
        "    # Please go through the reference notebook and research paper to complete the scoring functions\n",
        "\n",
        "    if self.scoring_function=='dot':\n",
        "      # Intialize variables needed for Dot score function here\n",
        "      \n",
        "      pass\n",
        "    if scoring_function == 'general':\n",
        "      # Intialize variables needed for General score function here\n",
        "      self.W=Dense(self.att_units)  \n",
        "      \n",
        "      pass\n",
        "    elif scoring_function == 'concat':\n",
        "      # Intialize variables needed for Concat score function here\n",
        "      self.W1=Dense(self.att_units,activation='tanh')\n",
        "      self.W2=Dense(self.att_units,activation='tanh')  \n",
        "    \n",
        "      self.V=Dense(1)\n",
        "      pass\n",
        "  \n",
        "  \n",
        "  def call(self,decoder_hidden_state,encoder_output):\n",
        "    '''\n",
        "      Attention mechanism takes two inputs current step -- decoder_hidden_state and all the encoder_outputs.\n",
        "      * Based on the scoring function we will find the score or similarity between decoder_hidden_state and encoder_output.\n",
        "        Multiply the score function with your encoder_outputs to get the context vector.\n",
        "        Function returns context vector and attention weights(softmax - scores)\n",
        "    '''\n",
        "    \n",
        "    if self.scoring_function == 'dot':\n",
        "        # Implement Dot score function here\n",
        "        #print(encoder_output.shape,decoder_hidden_state.shape)\n",
        "        decoder_hidden_state=tf.reshape(decoder_hidden_state,shape=(tf.shape(decoder_hidden_state)[0],1,tf.shape(decoder_hidden_state)[1]))\n",
        "        #print(decoder_hidden_state.shape[0])\n",
        "        #print(decoder_hidden_state.shape[1])\n",
        "        #print(decoder_hidden_state.shape)\n",
        "        score=tf.matmul(decoder_hidden_state,encoder_output,transpose_b=True)\n",
        "        \n",
        "        \n",
        "    elif self.scoring_function == 'general':\n",
        "        # Implement General score function here\n",
        "        decoder_hidden_state=tf.reshape(decoder_hidden_state,shape=(tf.shape(decoder_hidden_state)[0],1,tf.shape(decoder_hidden_state)[1]))\n",
        "        score=tf.matmul(decoder_hidden_state,self.W(encoder_output),transpose_b=True)\n",
        "       \n",
        "\n",
        "    elif self.scoring_function == 'concat':\n",
        "        # Implement General score function here\n",
        "        decoder=tf.expand_dims(decoder_hidden_state,1)\n",
        "        score=self.V(tf.nn.tanh(self.W1(decoder)+self.W2(encoder_output)))\n",
        "        attention_weights=tf.nn.softmax(score,axis=1)\n",
        "        context_vector=attention_weights*encoder_output\n",
        "        context_vector=tf.reduce_sum(context_vector,axis=1)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "        \n",
        "    attention_weights=tf.nn.softmax(score,axis=2)\n",
        "    context_vector=tf.matmul(attention_weights,encoder_output)\n",
        "    context_vector=tf.reshape(context_vector,shape=(tf.shape(context_vector)[0],tf.shape(context_vector)[2]))\n",
        "    attention_weights=tf.reshape(attention_weights,shape=(tf.shape(attention_weights)[0],tf.shape(attention_weights)[2],tf.shape(attention_weights)[1]))\n",
        "    return context_vector, attention_weights\n",
        "    \n",
        "    "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIc88R4fZnhQ"
      },
      "source": [
        "class One_Step_Decoder(tf.keras.Model):\n",
        "  def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
        "      super().__init__()\n",
        "      # Initialize decoder embedding layer, LSTM and any other objects needed\n",
        "      self.embedding = Embedding(input_dim=tar_vocab_size, output_dim=embedding_dim, input_length=input_length,\n",
        "                           mask_zero=True, name=\"embedding_layer\")\n",
        "      self.lstm=LSTM(dec_units,return_sequences=True,return_state=True)\n",
        "      self.attention=Attention(score_fun,att_units)\n",
        "      self.dense=tf.keras.layers.Dense(tar_vocab_size)\n",
        "\n",
        "\n",
        "  def call(self,input_to_decoder, encoder_output, state_h,state_c):\n",
        "    '''\n",
        "        One step decoder mechanisim step by step:\n",
        "      A. Pass the input_to_decoder to the embedding layer and then get the output(batch_size,1,embedding_dim)\n",
        "      B. Using the encoder_output and decoder hidden state, compute the context vector.\n",
        "      C. Concat the context vector with the step A output\n",
        "      D. Pass the Step-C output to LSTM/GRU and get the decoder output and states(hidden and cell state)\n",
        "      E. Pass the decoder output to dense layer(vocab size) and store the result into output.\n",
        "      F. Return the states from step D, output from Step E, attention weights from Step -B\n",
        "    '''\n",
        "    x=self.embedding(input_to_decoder)\n",
        "    context_vector,attention_weights=self.attention(state_h,encoder_output)\n",
        "    concat=tf.concat([x,tf.expand_dims(context_vector,1)],axis=-1)\n",
        "    output,decoder_state_h,decoder_state_c=self.lstm(x,initial_state=[state_h,state_c])\n",
        "    output=tf.reshape(output,(-1,output.shape[2]))\n",
        "    final_output=self.dense(output)\n",
        "    return final_output,decoder_state_h,decoder_state_c,attention_weights,context_vector\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdDU-hRWZr46"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self,out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
        "      #Intialize necessary variables and create an object from the class onestepdecoder\n",
        "      super(Decoder,self).__init__()\n",
        "      self.out_vocab_size=out_vocab_size\n",
        "      self.embedding_dim=embedding_dim\n",
        "      self.input_length=input_length\n",
        "      self.dec_units=dec_units\n",
        "      self.score_fun=score_fun\n",
        "      self.att_units=att_units\n",
        "      self.One_Step_Decoder=One_Step_Decoder(self.out_vocab_size, self.embedding_dim, self.input_length, self.dec_units ,self.score_fun ,self.att_units)\n",
        "        \n",
        "    def call(self, input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state ):\n",
        "\n",
        "        #Initialize an empty Tensor array, that will store the outputs at each and every time step\n",
        "        #Create a tensor array as shown in the reference notebook\n",
        "        \n",
        "        #Iterate till the length of the decoder input\n",
        "            # Call onestepdecoder for each token in decoder_input\n",
        "            # Store the output in tensorarray\n",
        "        # Return the tensor array\n",
        "        all_outputs=tf.TensorArray(tf.float32,size=tf.shape(input_to_decoder)[1])\n",
        "        for i in range(tf.shape(input_to_decoder)[1]):\n",
        "          output,decoder_hidden_state,decoder_cell_state,attention_weights,context_vector=self.One_Step_Decoder(input_to_decoder[:,i:i+1],encoder_output,decoder_hidden_state,decoder_cell_state)\n",
        "          \n",
        "          all_outputs=all_outputs.write(i,output)\n",
        "        all_outputs=tf.transpose(all_outputs.stack(),[1,0,2])\n",
        "        return all_outputs"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEEmVWx-Zxic"
      },
      "source": [
        "class encoder_decoder(tf.keras.Model):\n",
        "  def __init__(self,inp_vocab_size, embedding_dim,enc_units,input_length,out_vocab_size,dec_units ,score_fun ,att_units,batch_size):\n",
        "    #Intialize objects from encoder decoder\n",
        "    super().__init__()\n",
        "    self.batch_size=batch_size\n",
        "    self.encoder=Encoder(inp_vocab_size,embedding_dim,enc_units,input_length)\n",
        "    self.decoder=Decoder(out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units)\n",
        "\n",
        "  \n",
        "  def call(self,data):\n",
        "    #Intialize encoder states, Pass the encoder_sequence to the embedding layer\n",
        "    # Decoder initial states are encoder final states, Initialize it accordingly\n",
        "    # Pass the decoder sequence,encoder_output,decoder states to Decoder\n",
        "    # return the decoder output\n",
        "    \n",
        "    input,output=data[0],data[1]\n",
        "    initial_state=self.encoder.initialize_states(self.batch_size)\n",
        "    encoder_output,encoder_final_state_h,encoder_final_state_c=self.encoder(input,initial_state)\n",
        "    decoder_outputs=self.decoder(output,encoder_output,encoder_final_state_h,encoder_final_state_c)\n",
        "\n",
        "    return decoder_outputs"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7RqLpJmZ1tR"
      },
      "source": [
        "#https://www.tensorflow.org/tutorials/text/image_captioning#model\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    \"\"\" Custom loss function that will not consider the loss for padded zeros.\n",
        "    why are we using this, can't we use simple sparse categorical crossentropy?\n",
        "    Yes, you can use simple sparse categorical crossentropy as loss like we did in task-1. But in this loss function we are ignoring the loss\n",
        "    for the padded zeros. i.e when the input is zero then we donot need to worry what the output is. This padded zeros are added from our end\n",
        "    during preprocessing to make equal length for all the sentences.\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ircibD0GwOiz"
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "def accuracy(y_true, y_pred):\n",
        "\n",
        "    pred_value= K.cast(K.argmax(y_pred, axis=-1), dtype='float32')\n",
        "    true_value = K.cast(K.equal(y_true, pred_value), dtype='float32')\n",
        "\n",
        "    mask = K.cast(K.greater(y_true, 0), dtype='float32')\n",
        "    n_correct = K.sum(mask * true_value)\n",
        "    n_total = K.sum(mask)\n",
        "  \n",
        "    return n_correct / n_total"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyfPYVphaDv6"
      },
      "source": [
        "class Dataset:\n",
        "    def __init__(self, data, tok_corrupt, tok_uncorrupt, max_len):\n",
        "        self.encoder_inps = data['corrupted'].values\n",
        "        self.decoder_inps = data['uncorrupted_inp'].values\n",
        "        self.decoder_outs = data['uncorrupted_op'].values\n",
        "        self.tok_corrupt = tok_corrupt\n",
        "        self.tok_uncorrupt = tok_uncorrupt\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        self.encoder_seq = self.tok_corrupt.texts_to_sequences([self.encoder_inps[i]]) # need to pass list of values\n",
        "        self.decoder_inp_seq = self.tok_uncorrupt.texts_to_sequences([self.decoder_inps[i]])\n",
        "        self.decoder_out_seq = self.tok_uncorrupt.texts_to_sequences([self.decoder_outs[i]])\n",
        "\n",
        "        self.encoder_seq = pad_sequences(self.encoder_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
        "        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
        "        self.decoder_out_seq = pad_sequences(self.decoder_out_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
        "        return self.encoder_seq, self.decoder_inp_seq, self.decoder_out_seq\n",
        "\n",
        "    def __len__(self): # your model.fit_gen requires this function\n",
        "        return len(self.encoder_inps)\n",
        "\n",
        "    \n",
        "class Dataloder(tf.keras.utils.Sequence):    \n",
        "    def __init__(self, dataset, batch_size=1):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.indexes = np.arange(len(self.dataset.encoder_inps))\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        start = i * self.batch_size\n",
        "        stop = (i + 1) * self.batch_size\n",
        "        data = []\n",
        "        for j in range(start, stop):\n",
        "            data.append(self.dataset[j])\n",
        "\n",
        "        batch = [np.squeeze(np.stack(samples, axis=1), axis=0) for samples in zip(*data)]\n",
        "        return tuple([[batch[0],batch[1]],batch[2]])\n",
        "\n",
        "    def __len__(self):  # your model.fit_gen requires this function\n",
        "        return len(self.indexes) // self.batch_size\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.random.permutation(self.indexes)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iH7AMlWEqGwh"
      },
      "source": [
        "def model_inference(data1,data2):\n",
        "  '''the function takes single argument and performs all the preprocessing and feature\n",
        "  engineering on top of that and also performs modelling of data..\n",
        "  the function will return the predicted output of our task'''\n",
        "\n",
        "  #HERE THE FUNCTION ARGUMENT WILL TAKE A DATAFRAME AS AN INPUT\n",
        "\n",
        "  #PERFORMING ALL THE REQUIRED DATA PREPROCESSING\n",
        "  \n",
        "  def decontractions(phrase):\n",
        "    '''decontracted takes text and convert contractions into natural form.'''\n",
        "  #https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python/47091490#47091490\n",
        "  # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\n",
        "\n",
        "  # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "\n",
        "    return phrase\n",
        "  data1['corrupted']=data1['corrupted'].apply(lambda x:decontractions(x))\n",
        "  data1['uncorrupted']=data1['uncorrupted'].apply(lambda x:decontractions(x))\n",
        "\n",
        "  data2['corrupted']=data2['corrupted'].apply(lambda x:decontractions(x))\n",
        "  data2['uncorrupted']=data2['uncorrupted'].apply(lambda x:decontractions(x))\n",
        "\n",
        "  data1['corrupted']=data1['corrupted'].apply(lambda x:x.lower())\n",
        "  data1['uncorrupted']=data1['uncorrupted'].apply(lambda x:x.lower())\n",
        "\n",
        "  data2['corrupted']=data2['corrupted'].apply(lambda x:x.lower())\n",
        "  data2['uncorrupted']=data2['uncorrupted'].apply(lambda x:x.lower())\n",
        "\n",
        "  data1['corrupted_len'] = data1['corrupted'].str.split().apply(len)\n",
        "  data1 = data1[data1['corrupted_len'] < 25]\n",
        "\n",
        "  data1['uncorrupted_len'] = data1['uncorrupted'].str.split().apply(len)\n",
        "  data1 = data1[data1['uncorrupted_len'] < 25]\n",
        "\n",
        "  data2['corrupted_len'] = data2['corrupted'].str.split().apply(len)\n",
        "  data2 = data2[data2['corrupted_len'] < 25]\n",
        "\n",
        "  data2['uncorrupted_len'] = data2['uncorrupted'].str.split().apply(len)\n",
        "  data2 = data2[data2['uncorrupted_len'] < 25]\n",
        "\n",
        "  data1['uncorrupted_inp']=data1['uncorrupted'].apply(lambda x:('<START>'+' '+x))\n",
        "  data1['uncorrupted_op']=data1['uncorrupted'].apply(lambda x:(x+' '+'<END>'))\n",
        "  data1.drop(['uncorrupted'],axis=1,inplace=True)\n",
        "\n",
        "  data2['uncorrupted_inp']=data2['uncorrupted'].apply(lambda x:('<START>'+' '+x))\n",
        "  data2['uncorrupted_op']=data2['uncorrupted'].apply(lambda x:(x+' '+'<END>'))\n",
        "  data2.drop(['uncorrupted'],axis=1,inplace=True)\n",
        "\n",
        "  data1.drop(['corrupted_len','uncorrupted_len'],axis=1,inplace=True)\n",
        "  data2.drop(['corrupted_len','uncorrupted_len'],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  data=data1.sample(5000)\n",
        "  \n",
        "  train, validation = train_test_split(data, test_size=0.2)\n",
        "  train=pd.concat([train,data2.iloc[0:1500]],ignore_index=True).astype(str)\n",
        "  validation=pd.concat([validation,data2.iloc[1500:1675]],ignore_index=True).astype(str)\n",
        "\n",
        "\n",
        "  train.to_pickle('train.pkl')\n",
        "  validation.to_pickle('validation.pkl')\n",
        "\n",
        "  train=pd.read_pickle('train.pkl')\n",
        "  validation=pd.read_pickle('validation.pkl')\n",
        "\n",
        "  # for one sentence we will be adding <end> token so that the tokanizer learns the word <end>\n",
        "  # with this we can use only one tokenizer for both encoder output and decoder output\n",
        "  train.iloc[0]['uncorrupted_inp']= str(train.iloc[0]['uncorrupted_inp'])+' <end>'\n",
        "  train.iloc[0]['uncorrupted_out']= str(train.iloc[0]['uncorrupted_op'])+' <end>'\n",
        "\n",
        "  train.sample(frac=1)\n",
        "\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  tok_corrupted=Tokenizer(filters='#$%&()*+-/=@[\\\\]^_`{|}~\\t\\n')\n",
        "  tok_corrupted.fit_on_texts(train['corrupted'].values)\n",
        "\n",
        "  tok_uncorrupted=Tokenizer(filters='#$%&()*+-/=@[\\\\]^_`{|}~\\t\\n')\n",
        "  tok_uncorrupted.fit_on_texts(train['uncorrupted_inp'].values)\n",
        "\n",
        "  vocab_corrupt=len(tok_corrupted.word_index.keys())\n",
        "  vocab_uncorrupt=len(tok_uncorrupted.word_index.keys())\n",
        "\n",
        "  embeddings_index = dict()\n",
        "  f = open('glove.6B.100d.txt')\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  embedding_matrix = np.zeros((vocab_corrupt+1, 100))\n",
        "  for word, i in tok_corrupted.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "\n",
        "\n",
        "  train_dataset = Dataset(train, tok_corrupted, tok_uncorrupted,25 )\n",
        "  test_dataset  = Dataset(validation,tok_corrupted, tok_uncorrupted,25)\n",
        "\n",
        "  \n",
        "\n",
        "  train_dataloader = Dataloder(train_dataset, batch_size=64)\n",
        "  test_dataloader = Dataloder(test_dataset, batch_size=64)\n",
        "  \n",
        "  print(train_dataloader[0][0][0].shape, train_dataloader[0][0][1].shape, train_dataloader[0][1].shape)\n",
        "\n",
        "\n",
        "  import datetime\n",
        "  from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler,EarlyStopping,TerminateOnNaN,TensorBoard\n",
        "  earlystop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, verbose=1)\n",
        "  filepath=\"/content/drive/MyDrive/CaseStudy2/Model5/weights-{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
        "  checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_accuracy',  verbose=4, save_best_only=True, mode='auto')\n",
        "  logdir = \"/content/drive/MyDrive/CaseStudy2/Model5/Logs/fit_model2/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  train_summary_writer = tf.summary.create_file_writer(logdir)\n",
        "  tensorboard_callback = TensorBoard(log_dir=logdir,histogram_freq=1,profile_batch = 100000000)\n",
        "  callback_list = [checkpoint,tensorboard_callback]\n",
        "\n",
        "\n",
        "  model  = encoder_decoder(vocab_corrupt,100,300,25,vocab_uncorrupt,300,'concat',50,64)\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "  model.compile(optimizer=optimizer,loss=loss_function,metrics=[accuracy])\n",
        "  batch_size=64\n",
        "  train_steps=train.shape[0]//batch_size\n",
        "  valid_steps=validation.shape[0]//batch_size\n",
        "  model.fit_generator(train_dataloader, steps_per_epoch=train_steps, epochs=1, validation_data=train_dataloader, validation_steps=valid_steps)\n",
        "  model.summary()\n",
        "\n",
        "  #model.save_weights('model.h5')\n",
        "  model.load_weights('model.h5')\n",
        "\n",
        "  #https://matplotlib.org/3.1.1/gallery/ticks_and_spines/tick-locators.html\n",
        "  import matplotlib.pyplot as plt\n",
        "  import matplotlib.ticker as ticker\n",
        "\n",
        "  def plot_attention(attention, sentence, predicted_sentence):\n",
        "    #Refer: https://www.tensorflow.org/tutorials/text/nmt_with_attention#translate\n",
        "    \n",
        "    #attention_mat= attention[:len(predicted_sentence), :len(sentence)]\n",
        "    \n",
        "    fig = plt.figure(figsize=(6, 6))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1)) \n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show() \n",
        "\n",
        "  def predict(input_sentence):\n",
        "    input_vec = tok_corrupted.texts_to_sequences([input_sentence])\n",
        "    input_vec =  tf.keras.preprocessing.sequence.pad_sequences(input_vec,maxlen=25,padding='post')\n",
        "    input_vec = tf.convert_to_tensor(input_vec)\n",
        "    enc_initial_states = None\n",
        "    enc_out, h_state, c_state = model.layers[0](input_vec,enc_initial_states)\n",
        "    curr_vec = np.array(tok_uncorrupted.word_index[\"<start>\"])\n",
        "    curr_vec = curr_vec.reshape(1,1)\n",
        "    end_vec = np.array(tok_uncorrupted.word_index[\"<end>\"]).reshape(1,1)\n",
        "    result=''\n",
        "    atten_weights= np.zeros((25,25)) #(max_target_length, max_source_length)\n",
        "    for i in range(25):\n",
        "      decoder_output, lstm_state_h, lstm_state_c, atten_w, context_vectors = model.layers[1].One_Step_Decoder(curr_vec, enc_out, h_state, c_state)\n",
        "    \n",
        "      Result_beam_list=beam_search_decoder(decoder_output,k=1)\n",
        "      Result_beam=Result_beam_list[0][0]\n",
        "   \n",
        "      predicted_id = tf.argmax(decoder_output[0]).numpy()\n",
        "      \n",
        "    \n",
        "      if tok_uncorrupted.index_word[predicted_id] != '<end>':\n",
        "        result += tok_uncorrupted.index_word[Result_beam[0]] + ' '\n",
        "      # the predicted ID is fed back into the model\n",
        "      atten_w = tf.squeeze(atten_w, axis=-1) #to get shape as (batch, time_step)\n",
        "        \n",
        "      atten_w = tf.squeeze(atten_w, axis=0)\n",
        "      \n",
        "      atten_weights[i][:atten_w.shape[0]] = atten_w.numpy()\n",
        "      \n",
        "      h_state, c_state = lstm_state_h, lstm_state_c\n",
        "      curr_vec = tf.expand_dims([predicted_id], 0)\n",
        "    return result,input_sentence, atten_weights\n",
        "\n",
        "  def translate_sentence(inp_sentence):\n",
        "    result, input_sentence, attention_plot = predict(inp_sentence)\n",
        "    print('Input: %s' % (input_sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "    print(\"-\"*50)\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(input_sentence.split(' '))]\n",
        "    plot_attention(attention_plot, input_sentence.split(' '),    result.split(' '))\n",
        "    return result\n",
        "\n",
        "\n",
        "  from tqdm import tqdm\n",
        "  import nltk.translate.bleu_score as bleu\n",
        "  test=validation.sample(10)\n",
        "  score=[]\n",
        "  predicted=[]\n",
        "  test_inp=test['corrupted'].values\n",
        "\n",
        "  for i in tqdm(test_inp):\n",
        "  \n",
        "    pred,inp,att=predict(i)\n",
        "    predicted.append(pred)\n",
        "\n",
        "  for i in list(zip(test['uncorrupted_inp'].values,predicted)):\n",
        "    score.append(bleu.sentence_bleu([i[0].split(' ')[1:]],i[1].split(' ')))\n",
        "  \n",
        "  print(\"average bleu score:\",(sum(score)/len(score)))"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfkNC-20xz0W",
        "outputId": "f835aac7-ea3e-4a78-ba72-4ae272a933cc"
      },
      "source": [
        "model_inference(data1,data2)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 25) (64, 25) (64, 25)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85/85 [==============================] - 52s 486ms/step - loss: 4.0644 - accuracy: 0.0831 - val_loss: 3.7798 - val_accuracy: 0.0914\n",
            "Model: \"encoder_decoder_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_16 (Encoder)         multiple                  1362800   \n",
            "_________________________________________________________________\n",
            "decoder_16 (Decoder)         multiple                  3476746   \n",
            "=================================================================\n",
            "Total params: 4,839,546\n",
            "Trainable params: 4,839,546\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [16:16<00:00, 97.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average bleu score: 0.5253437739541507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    }
  ]
}